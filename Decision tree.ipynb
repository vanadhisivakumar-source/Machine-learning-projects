{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNsvq9US3cNLWuovuo0BUHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanadhisivakumar-source/Machine-learning-projects/blob/main/Decision%20tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHpAqp1qLI34"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def entropy(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    # Filter out zero probabilities to avoid log2(0) which is -inf\n",
        "    probabilities = counts / np.sum(counts)\n",
        "    entropy_val = np.sum([-p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
        "    return entropy_val\n",
        "\n",
        "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name])\n",
        "                               for i in range(len(vals))])\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "\n",
        "def ID3(data, originaldata, features, target_attribute_name=\"class\", parent_node_class=None):\n",
        "    # If all target values are the same, return that value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    # If the dataset is empty, return the most common target value from the original dataset\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
        "    # If there are no features left, return the parent's most common target value\n",
        "    elif len(features) == 0:\n",
        "        return parent_node_class\n",
        "    else:\n",
        "        # Determine the most common class in the current dataset (for parent_node_class)\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
        "\n",
        "        # Calculate Information Gain for each feature\n",
        "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]\n",
        "        # Select the feature with the highest Information Gain\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "\n",
        "        # Initialize the tree with the best feature as the root\n",
        "        tree = {best_feature: {}}\n",
        "\n",
        "        # Remove the best_feature from the list of features for recursive calls\n",
        "        remaining_features = [i for i in features if i != best_feature]\n",
        "\n",
        "        # Recursively build the tree for each value of the best_feature\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            subtree = ID3(sub_data, originaldata, remaining_features, target_attribute_name, parent_node_class)\n",
        "            tree[best_feature][value] = subtree\n",
        "\n",
        "        return tree\n",
        "\n",
        "# Main execution block\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('playtennis.csv', names=['outlook', 'temperature', 'humidity', 'wind', 'class'])\n",
        "\n",
        "# Define initial features (all columns except the target 'class')\n",
        "initial_features = dataset.columns.drop('class').tolist()\n",
        "\n",
        "# Build the decision tree\n",
        "tree = ID3(dataset, dataset, initial_features, target_attribute_name='class')\n",
        "\n",
        "# Display the built tree\n",
        "print('\\nDisplay Tree\\n', tree)"
      ]
    }
  ]
}