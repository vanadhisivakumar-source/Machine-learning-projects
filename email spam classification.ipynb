{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPBtNtU6ng/0VMARyvvunKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanadhisivakumar-source/Machine-learning-projects/blob/main/email%20spam%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qxux6oOaH4N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72430ec"
      },
      "source": [
        "# Task\n",
        "Implement a Naive Bayes classifier to classify a new email as 'spam' or 'not spam' based on a synthetic training dataset, then present the predicted classification and the reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9b79a99"
      },
      "source": [
        "## Create Training Dataset\n",
        "\n",
        "### Subtask:\n",
        "Generate a sample training dataset representing emails with features ('free', 'win', 'offer') and the target variable ('spam'). This will serve as the basis for training the Naive Bayes classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8160d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the pandas library and create a dictionary with the sample training data, including features ('free', 'win', 'offer') and the target variable ('spam'). This data will then be converted into a pandas DataFrame named `df_train` as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33981fe4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary to represent the synthetic training data\n",
        "data = {\n",
        "    'free': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "    'win': [0, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n",
        "    'offer': [0, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
        "    'spam': [0, 0, 1, 0, 0, 1, 0, 1, 1, 0] # 1 for spam, 0 for not spam\n",
        "}\n",
        "\n",
        "# Convert the dictionary into a pandas DataFrame\n",
        "df_train = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"Training DataFrame created:\")\n",
        "print(df_train.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd3dc12"
      },
      "source": [
        "## Calculate Probabilities\n",
        "\n",
        "### Subtask:\n",
        "Implement the Naive Bayes algorithm to calculate the prior probabilities (P(spam), P(not spam)) and likelihood probabilities (e.g., P(free|spam), P(win|not spam)) from the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6285bc9"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the Naive Bayes algorithm by calculating the prior and likelihood probabilities from the training data, following the instructions provided in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d31c9e3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Separate the training data into spam and not spam subsets\n",
        "df_spam = df_train[df_train['spam'] == 1]\n",
        "df_not_spam = df_train[df_train['spam'] == 0]\n",
        "\n",
        "# 2. Calculate the prior probability of 'spam'\n",
        "p_spam = len(df_spam) / len(df_train)\n",
        "\n",
        "# 3. Calculate the prior probability of 'not spam'\n",
        "p_not_spam = len(df_not_spam) / len(df_train)\n",
        "\n",
        "print(f\"P(spam): {p_spam:.2f}\")\n",
        "print(f\"P(not spam): {p_not_spam:.2f}\")\n",
        "\n",
        "# Features for calculating likelihoods\n",
        "features = ['free', 'win', 'offer']\n",
        "\n",
        "# Initialize dictionaries to store likelihoods\n",
        "likelihoods_spam = {}\n",
        "likelihoods_not_spam = {}\n",
        "\n",
        "# Calculate likelihoods for each feature\n",
        "for feature in features:\n",
        "    # 4. P(feature|spam)\n",
        "    likelihoods_spam[feature] = (df_spam[feature].sum() + 1) / (len(df_spam) + 2) # Adding 1 for Laplace smoothing (alpha=1, k=2 for binary features)\n",
        "    # 5. P(feature|not spam)\n",
        "    likelihoods_not_spam[feature] = (df_not_spam[feature].sum() + 1) / (len(df_not_spam) + 2) # Adding 1 for Laplace smoothing (alpha=1, k=2 for binary features)\n",
        "\n",
        "# 6. Store all calculated probabilities\n",
        "probabilities = {\n",
        "    'prior_spam': p_spam,\n",
        "    'prior_not_spam': p_not_spam,\n",
        "    'likelihoods_spam': likelihoods_spam,\n",
        "    'likelihoods_not_spam': likelihoods_not_spam\n",
        "}\n",
        "\n",
        "print(\"\\nLikelihoods given spam:\")\n",
        "for feature, prob in likelihoods_spam.items():\n",
        "    print(f\"P({feature}|spam): {prob:.2f}\")\n",
        "\n",
        "print(\"\\nLikelihoods given not spam:\")\n",
        "for feature, prob in likelihoods_not_spam.items():\n",
        "    print(f\"P({feature}|not spam): {prob:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27533081"
      },
      "source": [
        "## Predict New Email Classification\n",
        "\n",
        "### Subtask:\n",
        "Use the calculated probabilities to classify the example new email with features: 'free'=yes, 'win'=yes, 'offer'=no. This step will involve computing the posterior probabilities for both 'spam' and 'not spam' classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70588a3"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the new email's features and then calculate the unnormalized posterior probabilities for 'spam' and 'not spam' using the previously computed prior and likelihood probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6534e2cc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define the features of the new email\n",
        "new_email_features = {'free': 1, 'win': 1, 'offer': 0}\n",
        "\n",
        "# Retrieve probabilities from the 'probabilities' dictionary\n",
        "p_spam = probabilities['prior_spam']\n",
        "p_not_spam = probabilities['prior_not_spam']\n",
        "likelihoods_spam = probabilities['likelihoods_spam']\n",
        "likelihoods_not_spam = probabilities['likelihoods_not_spam']\n",
        "\n",
        "# 2. Calculate the unnormalized posterior probability for 'spam'\n",
        "# P(spam | new_email) = P(spam) * P(free|spam) * P(win|spam) * P(not offer|spam)\n",
        "posterior_spam_unnorm = p_spam\n",
        "for feature, value in new_email_features.items():\n",
        "    if value == 1: # Feature is present\n",
        "        posterior_spam_unnorm *= likelihoods_spam[feature]\n",
        "    else: # Feature is absent\n",
        "        posterior_spam_unnorm *= (1 - likelihoods_spam[feature])\n",
        "\n",
        "# 3. Calculate the unnormalized posterior probability for 'not spam'\n",
        "# P(not spam | new_email) = P(not spam) * P(free|not spam) * P(win|not spam) * P(not offer|not spam)\n",
        "posterior_not_spam_unnorm = p_not_spam\n",
        "for feature, value in new_email_features.items():\n",
        "    if value == 1: # Feature is present\n",
        "        posterior_not_spam_unnorm *= likelihoods_not_spam[feature]\n",
        "    else: # Feature is absent\n",
        "        posterior_not_spam_unnorm *= (1 - likelihoods_not_spam[feature])\n",
        "\n",
        "# 4. Print the unnormalized posterior probabilities\n",
        "print(f\"Unnormalized Posterior P(spam | new email): {posterior_spam_unnorm:.4f}\")\n",
        "print(f\"Unnormalized Posterior P(not spam | new email): {posterior_not_spam_unnorm:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9474480e"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the predicted classification (spam or not spam) for the new email, along with the reasoning based on the Naive Bayes probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901d1e87"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The new email, with features 'free'=yes, 'win'=yes, 'offer'=no, is classified as **spam**.\n",
        "\n",
        "This classification is based on the Naive Bayes probabilities:\n",
        "*   The unnormalized posterior probability of the email being 'spam' was calculated as approximately 0.0556.\n",
        "*   The unnormalized posterior probability of the email being 'not spam' was calculated as approximately 0.0469.\n",
        "\n",
        "Since P(spam | new\\_email) $>$ P(not spam | new\\_email), the email is classified as spam.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A synthetic training dataset (`df_train`) was created with 10 entries and 3 features ('free', 'win', 'offer') and a 'spam' target variable, all represented as integers.\n",
        "*   Prior probabilities were calculated from the training data: P(spam) = 0.40 and P(not spam) = 0.60.\n",
        "*   Likelihood probabilities for features given each class were calculated using Laplace smoothing (alpha=1):\n",
        "    *   **Given Spam:** P(free|spam) = 0.50, P(win|spam) = 0.83, P(offer|spam) = 0.67.\n",
        "    *   **Given Not Spam:** P(free|not spam) = 0.50, P(win|not spam) = 0.25, P(offer|not spam) = 0.38.\n",
        "*   For the new email ('free'=yes, 'win'=yes, 'offer'=no), the unnormalized posterior probabilities were:\n",
        "    *   Unnormalized P(spam | new email) \\$\\approx\\$ 0.0556.\n",
        "    *   Unnormalized P(not spam | new email) \\$\\approx\\$ 0.0469.\n",
        "*   Based on these probabilities, the Naive Bayes classifier predicted the new email to be 'spam'.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The Naive Bayes classifier successfully classified the new email based on the learned probabilities from the synthetic dataset. This demonstrates the basic application of the algorithm for text classification.\n",
        "*   To improve the model's accuracy and robustness, consider expanding the training dataset with more diverse examples and a larger vocabulary of features, potentially exploring different smoothing techniques or incorporating more advanced feature engineering.\n"
      ]
    }
  ]
}