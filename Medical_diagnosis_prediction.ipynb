{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNdj/uBb0KnsxkxXiT0lG9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanadhisivakumar-source/Machine-learning-projects/blob/main/Medical_diagnosis_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c473cf5"
      },
      "source": [
        "# Task\n",
        "Implement the Candidate Elimination Algorithm to find the Most Specific Hypothesis (S) and Most General Hypothesis (G) for the given dataset of Fever, Cough, and Fatigue attributes and the target Disease. Display the final version space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ed0950"
      },
      "source": [
        "## Define Data and Attributes\n",
        "\n",
        "### Subtask:\n",
        "Define the attributes (Fever, Cough, Fatigue) with their possible values and the target (Disease). Represent the given dataset as a list of examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c832e2"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the attributes with their possible values and then represent the given dataset as a list of examples in Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03bf1ada"
      },
      "source": [
        "attributes = {\n",
        "    'Fever': ['Yes', 'No'],\n",
        "    'Cough': ['Yes', 'No'],\n",
        "    'Fatigue': ['Yes', 'No']\n",
        "}\n",
        "\n",
        "dataset = [\n",
        "    {'Fever': 'Yes', 'Cough': 'No', 'Fatigue': 'Yes', 'Disease': 'Positive'},\n",
        "    {'Fever': 'Yes', 'Cough': 'Yes', 'Fatigue': 'No', 'Disease': 'Negative'},\n",
        "    {'Fever': 'No', 'Cough': 'Yes', 'Fatigue': 'Yes', 'Disease': 'Positive'},\n",
        "    {'Fever': 'No', 'Cough': 'No', 'Fatigue': 'No', 'Disease': 'Negative'},\n",
        "    {'Fever': 'Yes', 'Cough': 'Yes', 'Fatigue': 'Yes', 'Disease': 'Positive'}\n",
        "]\n",
        "\n",
        "print(\"Attributes and their possible values:\")\n",
        "for attr, values in attributes.items():\n",
        "    print(f\"  {attr}: {values}\")\n",
        "\n",
        "print(\"\\nDataset (first 3 examples):\")\n",
        "for i, example in enumerate(dataset[:3]):\n",
        "    print(f\"  Example {i+1}: {example}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13ed68f"
      },
      "source": [
        "## Initialize Hypotheses\n",
        "\n",
        "### Subtask:\n",
        "Initialize the Most Specific Hypothesis (S) with the first positive example and the Most General Hypothesis (G) with the most general possible hypothesis (all '?'s).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23d15c97"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to initialize the Most Specific Hypothesis (S) by finding the first positive example from the dataset and extracting its attribute values. Then, I will initialize the Most General Hypothesis (G) with '?' for each attribute. Finally, I will print both S and G.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36415ce0"
      },
      "source": [
        "initial_S = []\n",
        "for example in dataset:\n",
        "    if example['Disease'] == 'Positive':\n",
        "        initial_S = [example[attr] for attr in attributes.keys()]\n",
        "        break\n",
        "\n",
        "initial_G = ['?' for _ in attributes.keys()]\n",
        "\n",
        "print(f\"Initial Most Specific Hypothesis (S): {initial_S}\")\n",
        "print(f\"Initial Most General Hypothesis (G): {initial_G}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2f8321a"
      },
      "source": [
        "## Implement Candidate Elimination Algorithm\n",
        "\n",
        "### Subtask:\n",
        "Implement the candidate elimination algorithm. Iterate through the dataset examples. For each positive example, generalize S. For each negative example, specialize G, removing inconsistent hypotheses and adding new specialized ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d92836df"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will initialize the Most Specific Hypothesis (S) and Most General Hypothesis (G) lists, then define the `consistent` helper function as it's fundamental to the algorithm and will be used by other functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b77b76d0"
      },
      "source": [
        "S = [initial_S]\n",
        "G = [initial_G]\n",
        "attributes_keys = list(attributes.keys())\n",
        "attributes_values = attributes # Store for easy access to all possible values\n",
        "\n",
        "def consistent(hypothesis, example):\n",
        "    \"\"\"Checks if a hypothesis is consistent with an example.\"\"\"\n",
        "    for i, attr in enumerate(attributes_keys):\n",
        "        if hypothesis[i] == '?':\n",
        "            continue\n",
        "        if hypothesis[i] != example[attr]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "print(f\"Initial S: {S}\")\n",
        "print(f\"Initial G: {G}\")\n",
        "print(\"Defined consistent function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf29ebb8"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `is_more_general` helper function, which is crucial for comparing hypotheses in the Candidate Elimination Algorithm. This function will be used later when updating the S and G boundaries to ensure proper generalization and specialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "483d2e70"
      },
      "source": [
        "def is_more_general(h1, h2):\n",
        "    \"\"\"Checks if hypothesis h1 is more general than or equal to h2.\"\"\"\n",
        "    for i in range(len(h1)):\n",
        "        if h1[i] == h2[i]:\n",
        "            continue\n",
        "        elif h1[i] == '?':\n",
        "            continue\n",
        "        else:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "print(\"Defined is_more_general function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03d377d2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I need to define the `generalize_S` helper function, which will be responsible for updating the Most Specific Hypothesis (S) boundary. This function will iterate through each hypothesis in S, and if it's not consistent with a positive example, it will generalize it minimally until it becomes consistent, then remove any redundant hypotheses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db381227"
      },
      "source": [
        "def generalize_S(S, example, attributes_keys):\n",
        "    updated_S = []\n",
        "    for s_hypothesis in S:\n",
        "        if not consistent(s_hypothesis, example):\n",
        "            # Generalize s_hypothesis to be consistent with the positive example\n",
        "            new_s = list(s_hypothesis) # Create a mutable copy\n",
        "            for i, attr in enumerate(attributes_keys):\n",
        "                if new_s[i] == '?':\n",
        "                    continue\n",
        "                if new_s[i] != example[attr]:\n",
        "                    new_s[i] = '?' # Generalize by replacing with '?'\n",
        "            updated_S.append(new_s)\n",
        "        else:\n",
        "            updated_S.append(s_hypothesis)\n",
        "\n",
        "    # Remove more general hypotheses in S (i.e., keep only the most specific ones)\n",
        "    # This part needs careful implementation to ensure maximality within S\n",
        "    # For simplicity, we'll keep unique hypotheses for now and rely on later steps for stricter pruning\n",
        "    # A more robust implementation would involve checking for redundancy with is_more_general\n",
        "\n",
        "    # Remove duplicates\n",
        "    final_S = []\n",
        "    for h in updated_S:\n",
        "        if h not in final_S:\n",
        "            final_S.append(h)\n",
        "\n",
        "    # Additional filtering: Ensure no hypothesis in final_S is more general than another in final_S\n",
        "    # This is often handled implicitly by the construction or checked during final pruning.\n",
        "    # For basic CE, the above generalization handles the main logic for S.\n",
        "\n",
        "    return final_S\n",
        "\n",
        "print(\"Defined generalize_S function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bb78acc"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `specialize_G` helper function. This function will update the G-boundary by taking an inconsistent hypothesis with a negative example and generating all possible minimal specializations for it. It will also ensure that no hypothesis in G is more specific than another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0adbb1a4"
      },
      "source": [
        "def specialize_G(G, example, attributes_keys, attributes_values):\n",
        "    updated_G = []\n",
        "    for g_hypothesis in G:\n",
        "        if consistent(g_hypothesis, example):\n",
        "            # If g_hypothesis is consistent with a negative example, it must be specialized\n",
        "            for i, attr_key in enumerate(attributes_keys):\n",
        "                if g_hypothesis[i] == '?':\n",
        "                    # Specialize '?' to all possible specific values for this attribute\n",
        "                    for val in attributes_values[attr_key]:\n",
        "                        new_g = list(g_hypothesis)\n",
        "                        new_g[i] = val\n",
        "                        if not consistent(new_g, example) and is_more_general(new_g, s_hyp) for s_hyp in S:\n",
        "                            updated_G.append(new_g)\n",
        "                elif g_hypothesis[i] != example[attr_key]:\n",
        "                    # This part of the hypothesis is already inconsistent or specific and different, no specialization needed here\n",
        "                    pass # It means this specific value in g_hypothesis already makes it inconsistent with example[attr_key]\n",
        "        else:\n",
        "            updated_G.append(g_hypothesis)\n",
        "\n",
        "    # Filter out hypotheses that are more specific than others in updated_G (keep only maximally general)\n",
        "    final_G = []\n",
        "    for h1 in updated_G:\n",
        "        # Check if h1 is not more specific than any other h2 in updated_G\n",
        "        is_maximally_general = True\n",
        "        for h2 in updated_G:\n",
        "            if h1 != h2 and is_more_general(h2, h1):\n",
        "                is_maximally_general = False\n",
        "                break\n",
        "        if is_maximally_general:\n",
        "            final_G.append(h1)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_G = []\n",
        "    for h in final_G:\n",
        "        if h not in unique_G:\n",
        "            unique_G.append(h)\n",
        "\n",
        "    return unique_G\n",
        "\n",
        "print(\"Defined specialize_G function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4b9c90e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to a `SyntaxError` in the `if` condition within the `specialize_G` function. The generator expression `is_more_general(new_g, s_hyp) for s_hyp in S` was not properly wrapped in an `any()` or `all()` function. I need to wrap it in `any()` to correctly check if `new_g` is more general than at least one hypothesis in `S`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d0bc76c"
      },
      "source": [
        "def specialize_G(G, example, attributes_keys, attributes_values):\n",
        "    updated_G = []\n",
        "    for g_hypothesis in G:\n",
        "        if consistent(g_hypothesis, example):\n",
        "            # If g_hypothesis is consistent with a negative example, it must be specialized\n",
        "            for i, attr_key in enumerate(attributes_keys):\n",
        "                if g_hypothesis[i] == '?':\n",
        "                    # Specialize '?' to all possible specific values for this attribute\n",
        "                    for val in attributes_values[attr_key]:\n",
        "                        new_g = list(g_hypothesis)\n",
        "                        new_g[i] = val\n",
        "                        # The new specialization new_g must be:\n",
        "                        # 1. Inconsistent with the current negative example\n",
        "                        # 2. More general than at least one hypothesis in S\n",
        "                        if not consistent(new_g, example) and any(is_more_general(new_g, s_hyp) for s_hyp in S):\n",
        "                            updated_G.append(new_g)\n",
        "                # No elif for `g_hypothesis[i] != example[attr_key]` here because if it's not '?'\n",
        "                # and it's already inconsistent, it doesn't need to be specialized further at this position.\n",
        "        else:\n",
        "            # If g_hypothesis is already inconsistent with the negative example, keep it.\n",
        "            updated_G.append(g_hypothesis)\n",
        "\n",
        "    # Filter out hypotheses that are more specific than others in updated_G (keep only maximally general)\n",
        "    final_G = []\n",
        "    for h1 in updated_G:\n",
        "        is_maximally_general = True\n",
        "        for h2 in updated_G:\n",
        "            # h2 is more general than h1, so h1 is redundant\n",
        "            if h1 != h2 and is_more_general(h2, h1):\n",
        "                is_maximally_general = False\n",
        "                break\n",
        "        if is_maximally_general:\n",
        "            final_G.append(h1)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_G = []\n",
        "    for h in final_G:\n",
        "        if h not in unique_G:\n",
        "            unique_G.append(h)\n",
        "\n",
        "    return unique_G\n",
        "\n",
        "print(\"Defined specialize_G function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceee814c"
      },
      "source": [
        "**Reasoning**:\n",
        "I have defined all the necessary helper functions (`consistent`, `is_more_general`, `generalize_S`, `specialize_G`). Now I need to implement the main loop of the Candidate Elimination Algorithm by iterating through each example in the dataset and updating S and G boundaries based on whether the example is positive or negative. After the loop, I will print the final S and G hypotheses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ebc3a86"
      },
      "source": [
        "for k, example_dict in enumerate(dataset):\n",
        "    example = [example_dict[attr] for attr in attributes_keys]\n",
        "    target = example_dict['Disease']\n",
        "\n",
        "    print(f\"\\nProcessing Example {k+1}: {example_dict}\")\n",
        "\n",
        "    if target == 'Positive':\n",
        "        # Remove any hypotheses from G that are not consistent with the positive example\n",
        "        G = [g for g in G if consistent(g, example_dict)]\n",
        "\n",
        "        # Generalize S hypotheses\n",
        "        updated_S = []\n",
        "        for s_hyp in S:\n",
        "            if not consistent(s_hyp, example_dict):\n",
        "                # Generalize s_hyp minimally to cover the positive example\n",
        "                new_s = list(s_hyp)\n",
        "                for i in range(len(new_s)):\n",
        "                    if new_s[i] != '?' and new_s[i] != example[i]:\n",
        "                        new_s[i] = '?'\n",
        "                updated_S.append(new_s)\n",
        "            else:\n",
        "                updated_S.append(s_hyp)\n",
        "\n",
        "        # Filter S: remove redundant (more general) hypotheses and duplicates\n",
        "        final_S = []\n",
        "        for h1 in updated_S:\n",
        "            is_redundant = False\n",
        "            for h2 in updated_S:\n",
        "                if h1 != h2 and is_more_general(h2, h1): # h2 is more general than h1, so h1 is redundant\n",
        "                    is_redundant = True\n",
        "                    break\n",
        "            if not is_redundant and h1 not in final_S:\n",
        "                final_S.append(h1)\n",
        "        S = final_S\n",
        "\n",
        "        # Prune G if any G hypothesis is not more general than an S hypothesis\n",
        "        G = [g for g in G if any(is_more_general(g, s) for s in S)]\n",
        "\n",
        "    elif target == 'Negative':\n",
        "        # Remove any hypotheses from S that are consistent with the negative example\n",
        "        S = [s for s in S if not consistent(s, example_dict)]\n",
        "\n",
        "        # Specialize G hypotheses\n",
        "        updated_G = []\n",
        "        for g_hyp in G:\n",
        "            if consistent(g_hyp, example_dict):\n",
        "                # If g_hyp is consistent with a negative example, it must be specialized\n",
        "                for i, attr_key in enumerate(attributes_keys):\n",
        "                    if g_hyp[i] == '?':\n",
        "                        for val in attributes_values[attr_key]:\n",
        "                            if val != example[i]: # Only specialize to values that differ from the negative example\n",
        "                                new_g = list(g_hyp)\n",
        "                                new_g[i] = val\n",
        "                                # Ensure new_g is still more general than at least one S hypothesis\n",
        "                                if any(is_more_general(new_g, s_hyp) for s_hyp in S):\n",
        "                                    updated_G.append(new_g)\n",
        "            else:\n",
        "                updated_G.append(g_hyp)\n",
        "\n",
        "        # Filter G: remove redundant (more specific) hypotheses and duplicates\n",
        "        final_G = []\n",
        "        for h1 in updated_G:\n",
        "            is_redundant = False\n",
        "            for h2 in updated_G:\n",
        "                if h1 != h2 and is_more_general(h1, h2): # h1 is more general than h2, so h2 is redundant relative to h1\n",
        "                    pass # This logic needs to keep maximally general hypotheses. It should be: if h2 is MORE general than h1, then h1 is redundant.\n",
        "\n",
        "            is_maximally_general = True\n",
        "            for h2 in updated_G:\n",
        "                if h1 != h2 and is_more_general(h2, h1): # h2 is more general than h1, so h1 is not maximally general\n",
        "                    is_maximally_general = False\n",
        "                    break\n",
        "            if is_maximally_general and h1 not in final_G:\n",
        "                final_G.append(h1)\n",
        "        G = final_G\n",
        "\n",
        "        # Prune S if any S hypothesis is not more specific than any G hypothesis (should not happen with correctly maintained G)\n",
        "        S = [s for s in S if any(is_more_general(g, s) for g in G)]\n",
        "\n",
        "    print(f\"Current S: {S}\")\n",
        "    print(f\"Current G: {G}\")\n",
        "\n",
        "# Final pruning and consistency checks (if needed, typically handled in loops)\n",
        "final_S = []\n",
        "for s_hyp in S:\n",
        "    if any(is_more_general(g_hyp, s_hyp) for g_hyp in G):\n",
        "        final_S.append(s_hyp)\n",
        "S = final_S\n",
        "\n",
        "final_G = []\n",
        "for g_hyp in G:\n",
        "    if any(is_more_general(g_hyp, s_hyp) for s_hyp in S):\n",
        "        final_G.append(g_hyp)\n",
        "G = final_G\n",
        "\n",
        "# Remove any hypotheses from S that are more general than another in S\n",
        "S = [h1 for h1 in S if not any(is_more_general(h2, h1) and h1 != h2 for h2 in S)]\n",
        "# Remove any hypotheses from G that are more specific than another in G\n",
        "G = [h1 for h1 in G if not any(is_more_general(h1, h2) and h1 != h2 for h2 in G)]\n",
        "\n",
        "print(\"\\nFinal Version Space:\")\n",
        "print(f\"Most Specific Hypothesis (S): {S}\")\n",
        "print(f\"Most General Hypothesis (G): {G}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da802e98"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the implemented algorithm and the resulting final version space (S and G hypotheses).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17edea02"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The final version space determined by the Candidate Elimination Algorithm consists of:\n",
        "*   Most Specific Hypothesis (S): `[['?', '?', 'Yes']]`\n",
        "*   Most General Hypothesis (G): `[['?', '?', 'Yes']]`\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The dataset consists of 5 examples, each described by three attributes: 'Fever', 'Cough', and 'Fatigue', with a binary target 'Disease' ('Positive' or 'Negative').\n",
        "*   The Candidate Elimination Algorithm was initialized with the Most Specific Hypothesis (S) as `['Yes', 'No', 'Yes']` (derived from the first positive example) and the Most General Hypothesis (G) as `['?', '?', '?']`.\n",
        "*   The algorithm successfully processed each example, iteratively refining the S and G boundaries. For positive examples, S hypotheses were generalized, and G hypotheses inconsistent with the example were removed. For negative examples, S hypotheses consistent with the example were removed, and G hypotheses were specialized.\n",
        "*   After processing all examples, the algorithm converged, and both the Most Specific Hypothesis (S) and the Most General Hypothesis (G) were identical: `[['?', '?', 'Yes']]`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The convergence of S and G to `[['?', '?', 'Yes']]` indicates that the learned concept is uniquely identified by the dataset. This hypothesis suggests that 'Fatigue: Yes' is the sole condition for a 'Positive' disease diagnosis, while 'Fever' and 'Cough' attributes are irrelevant.\n",
        "*   Further validation could involve testing this learned hypothesis on new, unseen data to confirm its predictive accuracy and generalizability beyond the provided training set.\n"
      ]
    }
  ]
}