{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPTn8Kb7gBUJFWwZnBy5GGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanadhisivakumar-source/Machine-learning-projects/blob/main/handwritten_digit_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a6-WGQWnmcp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04903a7"
      },
      "source": [
        "# Task\n",
        "Implement an Artificial Neural Network with backpropagation for handwritten digit recognition using the MNIST dataset, covering data loading and preprocessing, defining the network architecture, training the model, evaluating its performance, and visualizing predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec270a1"
      },
      "source": [
        "## Load and Preprocess Data\n",
        "\n",
        "### Subtask:\n",
        "Load the handwritten digit dataset (e.g., MNIST). Normalize pixel values to a [0, 1] range and reshape images to a 1D array of 784 features. Convert labels to one-hot encoded format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f7e6623"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the necessary libraries, TensorFlow and Keras, to load and preprocess the MNIST dataset as specified in the instructions. This is a prerequisite for all subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cd6b016"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"TensorFlow and Keras modules loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30362842"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I will now load the MNIST dataset, normalize pixel values, reshape the images, and convert the labels to one-hot encoded format in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9b9651b"
      },
      "source": [
        "print(\"Loading MNIST dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"MNIST dataset loaded.\")\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "print(\"Pixel values normalized.\")\n",
        "\n",
        "# Reshape images to 1D array of 784 features\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
        "print(\"Images reshaped to 1D array.\")\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print(\"Labels converted to one-hot encoded format.\")\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6fba644"
      },
      "source": [
        "## Define ANN Architecture\n",
        "\n",
        "### Subtask:\n",
        "Define the Artificial Neural Network architecture with an input layer of 784 neurons, a hidden layer of 128 neurons with a suitable activation function (e.g., ReLU), and an output layer of 10 neurons with a Softmax activation function for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91d01013"
      },
      "source": [
        "**Reasoning**:\n",
        "To define the ANN architecture as per the subtask, I will import necessary Keras modules, create a Sequential model, add the specified Dense layers with their respective activation functions and input shape, and then print the model summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ff49377"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "print(\"Building the Artificial Neural Network architecture...\")\n",
        "\n",
        "# Create a Sequential model instance\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Dense layer for the hidden layer\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,))) # 784 neurons for input layer, 128 for hidden, ReLU activation\n",
        "\n",
        "# Add a Dense layer for the output layer\n",
        "model.add(Dense(10, activation='softmax')) # 10 neurons for output layer (10 digits), Softmax activation\n",
        "\n",
        "print(\"ANN architecture built successfully.\")\n",
        "\n",
        "# Print a summary of the model\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8526ef00"
      },
      "source": [
        "## Implement Backpropagation\n",
        "\n",
        "### Subtask:\n",
        "Implement the backpropagation algorithm for training the ANN. This includes defining the loss function (e.g., Categorical Crossentropy) and an optimizer (e.g., Stochastic Gradient Descent or Adam) to update weights and biases based on the calculated gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a601510d"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I will compile the previously defined ANN model using the specified Adam optimizer, categorical crossentropy loss function, and accuracy metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "428db53b"
      },
      "source": [
        "print(\"Compiling the model...\")\n",
        "\n",
        "# Compile the model with the Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bae07b29"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'model' object was not defined because the preceding cell, which built the model architecture, was not executed. To resolve the `NameError`, I need to include the model's definition along with the compilation step in this code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc679306"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "print(\"Building the Artificial Neural Network architecture...\")\n",
        "\n",
        "# Create a Sequential model instance\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Dense layer for the hidden layer\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,))) # 784 neurons for input layer, 128 for hidden, ReLU activation\n",
        "\n",
        "# Add a Dense layer for the output layer\n",
        "model.add(Dense(10, activation='softmax')) # 10 neurons for output layer (10 digits), Softmax activation\n",
        "\n",
        "print(\"ANN architecture built successfully.\")\n",
        "\n",
        "# Print a summary of the model\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCompiling the model...\")\n",
        "\n",
        "# Compile the model with the Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478137af"
      },
      "source": [
        "**Reasoning**:\n",
        "The next logical step in implementing backpropagation, as described in the subtask, is to train the model using the preprocessed data. This involves calling the `fit` method on the compiled model with the training data, specifying epochs and batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "993591a3"
      },
      "source": [
        "print(\"Training the model...\")\n",
        "\n",
        "# Train the model\n",
        "# Using x_train and y_train from the data preprocessing step\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,       # Number of times to iterate over the entire dataset\n",
        "    batch_size=128,  # Number of samples per gradient update\n",
        "    validation_split=0.2 # Use 20% of the training data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42f767ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The `NameError: name 'x_train' is not defined` indicates that the data loading and preprocessing steps, which define `x_train` and `y_train`, were not executed or their state was lost. To resolve this, I will re-include the data loading, normalization, reshaping, and one-hot encoding logic directly before the model training step to ensure all necessary variables are defined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a934fe03"
      },
      "source": [
        "print(\"Loading MNIST dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"MNIST dataset loaded.\")\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "print(\"Pixel values normalized.\")\n",
        "\n",
        "# Reshape images to 1D array of 784 features\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
        "print(\"Images reshaped to 1D array.\")\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print(\"Labels converted to one-hot encoded format.\")\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nTraining the model...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,       # Number of times to iterate over the entire dataset\n",
        "    batch_size=128,  # Number of samples per gradient update\n",
        "    validation_split=0.2 # Use 20% of the training data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23150e78"
      },
      "source": [
        "**Reasoning**:\n",
        "The `NameError: name 'mnist' is not defined` occurred because the necessary `tensorflow.keras.datasets` and other Keras modules were not imported in the current execution environment. To resolve this, I will combine all the required import statements, data loading and preprocessing steps, model definition, model compilation, and model training into a single robust code block. This ensures that all dependencies are met and variables are defined sequentially before use, addressing all previous `NameError` issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0739c26f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "print(\"Loading MNIST dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"MNIST dataset loaded.\")\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "print(\"Pixel values normalized.\")\n",
        "\n",
        "# Reshape images to 1D array of 784 features\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
        "print(\"Images reshaped to 1D array.\")\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print(\"Labels converted to one-hot encoded format.\")\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\nBuilding the Artificial Neural Network architecture...\")\n",
        "\n",
        "# Create a Sequential model instance\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Dense layer for the hidden layer\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,))) # 784 neurons for input layer, 128 for hidden, ReLU activation\n",
        "\n",
        "# Add a Dense layer for the output layer\n",
        "model.add(Dense(10, activation='softmax')) # 10 neurons for output layer (10 digits), Softmax activation\n",
        "\n",
        "print(\"ANN architecture built successfully.\")\n",
        "\n",
        "# Print a summary of the model\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCompiling the model...\")\n",
        "\n",
        "# Compile the model with the Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully.\")\n",
        "\n",
        "print(\"\\nTraining the model...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,       # Number of times to iterate over the entire dataset\n",
        "    batch_size=128,  # Number of samples per gradient update\n",
        "    validation_split=0.2 # Use 20% of the training data for validation\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b010d3af"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully completed the model training. The next logical step, according to the overall task, is to evaluate the trained model's performance on unseen data (the test set). This will involve using the `evaluate` method of the Keras model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7804d6"
      },
      "source": [
        "print(\"Evaluating the model...\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Model evaluation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23f48c82"
      },
      "source": [
        "## Visualize Predictions\n",
        "\n",
        "### Subtask:\n",
        "Display a few example handwritten digits from the test set along with the model's predicted digit and the actual digit to visually inspect the model's performance. Include legends for clarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce72f65b"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the model's performance, I will generate Python code that selects a few random test images, uses the trained model to predict their digits, and then displays each image along with its actual and predicted labels. This requires importing `matplotlib` for plotting and `numpy` for array manipulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef4ebbfd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Visualizing a few predictions...\")\n",
        "\n",
        "# Select a few random indices from the test dataset\n",
        "num_examples = 9\n",
        "random_indices = np.random.choice(len(x_test), num_examples, replace=False)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i, index in enumerate(random_indices):\n",
        "    # Retrieve the original image (reshaping the 1D x_test back to 28x28 for display)\n",
        "    image = x_test[index].reshape(28, 28)\n",
        "\n",
        "    # Retrieve the true one-hot encoded label and convert it back to a single digit\n",
        "    true_label = np.argmax(y_test[index])\n",
        "\n",
        "    # Use the trained model to make a prediction on the 1D reshaped test image\n",
        "    # We need to pass the input in a batch format, hence x_test[index:index+1]\n",
        "    prediction_probabilities = model.predict(x_test[index:index+1], verbose=0)\n",
        "\n",
        "    # Convert the model's prediction into a single predicted digit\n",
        "    predicted_label = np.argmax(prediction_probabilities)\n",
        "\n",
        "    # Create a subplot for each example\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image, cmap='gray') # Display the image\n",
        "\n",
        "    # Set the title with Actual and Predicted digits\n",
        "    color = 'green' if predicted_label == true_label else 'red'\n",
        "    plt.title(f'Actual: {true_label}\\nPredicted: {predicted_label}', color=color)\n",
        "\n",
        "    # Turn off the axis ticks for cleaner visualization\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout() # Adjust the layout of the plots to prevent overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"Prediction visualization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c526854d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the implementation of the ANN with backpropagation for handwritten digit recognition, including the model's architecture, training process, and evaluation results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf3433a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The implementation of the Artificial Neural Network (ANN) with backpropagation for handwritten digit recognition involved the following:\n",
        "\n",
        "*   **Model's Architecture**: The ANN consisted of an input layer of 784 neurons, a hidden layer with 128 neurons using a ReLU activation function, and an output layer with 10 neurons (for 10 classes) using a Softmax activation function. This architecture resulted in a total of 101,770 trainable parameters.\n",
        "*   **Training Process**:\n",
        "    *   The model was compiled using the Adam optimizer, 'categorical_crossentropy' as the loss function, and 'accuracy' as the metric.\n",
        "    *   Training was performed for 10 epochs with a batch size of 128, utilizing a 20% validation split.\n",
        "    *   During training, the model showed continuous improvement, with training accuracy increasing and loss decreasing.\n",
        "*   **Evaluation Results**:\n",
        "    *   After training, the model achieved a Test Loss of 0.0757 and a Test Accuracy of 0.9761 on the unseen test dataset.\n",
        "    *   Visual inspection of predictions confirmed that the model generally performed well, correctly identifying most digits.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The MNIST dataset was successfully loaded and preprocessed: pixel values were normalized to the \\[0, 1] range, images were reshaped from (28, 28) to a 1D array of 784 features, and labels were one-hot encoded for 10 classes.\n",
        "*   An Artificial Neural Network was defined with an input layer (784 neurons), a hidden layer (128 neurons, ReLU activation), and an output layer (10 neurons, Softmax activation), comprising a total of 101,770 trainable parameters.\n",
        "*   The model was compiled with the Adam optimizer and categorical cross-entropy loss, then trained for 10 epochs with a batch size of 128 and a 20% validation split.\n",
        "*   Upon evaluation, the trained model demonstrated strong performance on the test set, achieving a Test Loss of approximately 0.0757 and a Test Accuracy of 0.9761.\n",
        "*   Visualizations of random test images confirmed the model's ability to accurately predict handwritten digits, with correct predictions highlighted in green and incorrect ones in red.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The ANN model achieved a high accuracy of over 97% on the MNIST dataset, indicating its effectiveness in handwritten digit recognition. Further improvements could involve experimenting with more complex architectures (e.g., adding more hidden layers, increasing neuron count) or trying different optimizers and learning rates.\n",
        "*   While the current model performs well, analyzing specific misclassified digits (the 'red' predictions in the visualization) could provide insights into patterns of errors and inform strategies for targeted model enhancement or data augmentation.\n"
      ]
    }
  ]
}